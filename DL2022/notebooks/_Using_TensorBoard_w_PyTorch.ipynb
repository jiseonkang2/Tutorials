{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A Complete Guide to Using TensorBoard with PyTorch](https://towardsdatascience.com/a-complete-guide-to-using-tensorboard-with-pytorch-53cb2301e8c3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "torch.set_printoptions(linewidth=120)\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    ''' get the number of correct labels after training of the model'''\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data and creating the train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Images And Graphs with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = SummaryWriter()\n",
    "model = CNN()\n",
    "images, labels = next(iter(train_loader))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "tb.add_image(\"images\", grid)\n",
    "tb.add_graph(model, images)\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tendorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop to visualize Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct: 5834 loss 1382.1988372802734\n",
      "epoch: 1 total_correct: 6066 loss 1382.0108075141907\n",
      "epoch: 2 total_correct: 6062 loss 1381.9321489334106\n",
      "epoch: 3 total_correct: 6098 loss 1382.0273213386536\n",
      "epoch: 4 total_correct: 5943 loss 1382.053212404251\n",
      "epoch: 5 total_correct: 5969 loss 1382.074637413025\n",
      "epoch: 6 total_correct: 5915 loss 1382.0701925754547\n",
      "epoch: 7 total_correct: 6014 loss 1382.0029339790344\n",
      "epoch: 8 total_correct: 6019 loss 1381.9855835437775\n",
      "epoch: 9 total_correct: 5846 loss 1382.0712904930115\n"
     ]
    }
   ],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN().to(device)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)\n",
    "optimizer = opt.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "tb = SummaryWriter()\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    # for batch_id, (images, labels) in enumerate(train_loader):\n",
    "    for iamges, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        preds = model(images)\n",
    "\n",
    "        loss = criterion(preds, labels)\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # tb.add_scalar(\"Loss\", total_loss, batch_id)\n",
    "    tb.add_scalar(\"Loss\", total_loss, epoch)\n",
    "    tb.add_scalar(\"Correct\", total_correct, epoch)\n",
    "    tb.add_scalar(\"Accuracy\", total_correct/len(train_set), epoch)\n",
    "\n",
    "    tb.add_histogram(\"conv1.bias\", model.conv1.bias, epoch)\n",
    "    tb.add_histogram(\"conv1.weight\", model.conv1.weight, epoch)\n",
    "    tb.add_histogram(\"conv2.bias\", model.conv2.bias, epoch)\n",
    "    tb.add_histogram(\"conv2.weight\", model.conv2.weight, epoch)\n",
    "    # for name, weight in model.named_parameters():\n",
    "    #     tb.add_histogram(name, weight, epoch)\n",
    "    #     tb.add_histogram(f\"{name}.grad\", weight.grad, epoch)\n",
    "\n",
    "    print(\"epoch:\", epoch, \"total_correct:\", total_correct, \"loss\", total_loss)\n",
    "\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyterparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01, 0.001], [32, 64, 128], [True, False]]\n",
      "0.01 32 True\n",
      "0.01 32 False\n",
      "0.01 64 True\n",
      "0.01 64 False\n",
      "0.01 128 True\n",
      "0.01 128 False\n",
      "0.001 32 True\n",
      "0.001 32 False\n",
      "0.001 64 True\n",
      "0.001 64 False\n",
      "0.001 128 True\n",
      "0.001 128 False\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "parameters = dict(\n",
    "    lr = [0.01, 0.001],\n",
    "    batch_size = [32, 64, 128],\n",
    "    shuffle = [True, False]\n",
    ")\n",
    "\n",
    "param_values = [v for v in parameters.values()]\n",
    "print(param_values)\n",
    "\n",
    "for lr, batch_size, shuffle in product(*param_values):\n",
    "    print(lr, batch_size, shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id: 1\n",
      "batch_size: 32 lr: 0.01 shuffle: True\n",
      "epoch: 0 total_correct 47888 loss: 1018.841361835599\n",
      "batch_size: 32 lr: 0.01 shuffle: True\n",
      "epoch: 1 total_correct 50730 loss: 780.9257594048977\n",
      "batch_size: 32 lr: 0.01 shuffle: True\n",
      "epoch: 2 total_correct 51298 loss: 750.2692514099181\n",
      "batch_size: 32 lr: 0.01 shuffle: True\n",
      "epoch: 3 total_correct 51575 loss: 730.8675047084689\n",
      "batch_size: 32 lr: 0.01 shuffle: True\n",
      "epoch: 4 total_correct 51598 loss: 730.834707390517\n",
      "_______________________________________________________\n",
      "run id: 2\n",
      "batch_size: 32 lr: 0.01 shuffle: False\n",
      "epoch: 0 total_correct 46656 loss: 1102.8294258713722\n",
      "batch_size: 32 lr: 0.01 shuffle: False\n",
      "epoch: 1 total_correct 49949 loss: 848.2553975880146\n",
      "batch_size: 32 lr: 0.01 shuffle: False\n",
      "epoch: 2 total_correct 50205 loss: 824.955286026001\n",
      "batch_size: 32 lr: 0.01 shuffle: False\n",
      "epoch: 3 total_correct 50602 loss: 803.3780739232898\n",
      "batch_size: 32 lr: 0.01 shuffle: False\n",
      "epoch: 4 total_correct 50680 loss: 794.6867438405752\n",
      "_______________________________________________________\n",
      "run id: 3\n",
      "batch_size: 64 lr: 0.01 shuffle: True\n",
      "epoch: 0 total_correct 47920 loss: 506.08156718313694\n",
      "batch_size: 64 lr: 0.01 shuffle: True\n",
      "epoch: 1 total_correct 51334 loss: 364.4142320305109\n",
      "batch_size: 64 lr: 0.01 shuffle: True\n",
      "epoch: 2 total_correct 51919 loss: 341.04155719280243\n",
      "batch_size: 64 lr: 0.01 shuffle: True\n",
      "epoch: 3 total_correct 52242 loss: 330.0971336737275\n",
      "batch_size: 64 lr: 0.01 shuffle: True\n",
      "epoch: 4 total_correct 52487 loss: 320.1323035210371\n",
      "_______________________________________________________\n",
      "run id: 4\n",
      "batch_size: 64 lr: 0.01 shuffle: False\n",
      "epoch: 0 total_correct 46779 loss: 546.5395230799913\n",
      "batch_size: 64 lr: 0.01 shuffle: False\n",
      "epoch: 1 total_correct 50931 loss: 389.1332034468651\n",
      "batch_size: 64 lr: 0.01 shuffle: False\n",
      "epoch: 2 total_correct 51538 loss: 360.79968179762363\n",
      "batch_size: 64 lr: 0.01 shuffle: False\n",
      "epoch: 3 total_correct 51792 loss: 347.92504638433456\n",
      "batch_size: 64 lr: 0.01 shuffle: False\n",
      "epoch: 4 total_correct 51888 loss: 340.6284528672695\n",
      "_______________________________________________________\n",
      "run id: 5\n",
      "batch_size: 128 lr: 0.01 shuffle: True\n",
      "epoch: 0 total_correct 46200 loss: 283.0224460065365\n",
      "batch_size: 128 lr: 0.01 shuffle: True\n",
      "epoch: 1 total_correct 51098 loss: 185.81670811772346\n",
      "batch_size: 128 lr: 0.01 shuffle: True\n",
      "epoch: 2 total_correct 52068 loss: 166.43385046720505\n",
      "batch_size: 128 lr: 0.01 shuffle: True\n",
      "epoch: 3 total_correct 52457 loss: 156.076101988554\n",
      "batch_size: 128 lr: 0.01 shuffle: True\n",
      "epoch: 4 total_correct 52617 loss: 151.67123945057392\n",
      "_______________________________________________________\n",
      "run id: 6\n",
      "batch_size: 128 lr: 0.01 shuffle: False\n",
      "epoch: 0 total_correct 47410 loss: 259.25586012005806\n",
      "batch_size: 128 lr: 0.01 shuffle: False\n",
      "epoch: 1 total_correct 51936 loss: 171.71374809741974\n",
      "batch_size: 128 lr: 0.01 shuffle: False\n",
      "epoch: 2 total_correct 52526 loss: 157.33841934800148\n",
      "batch_size: 128 lr: 0.01 shuffle: False\n",
      "epoch: 3 total_correct 52796 loss: 151.07940676808357\n",
      "batch_size: 128 lr: 0.01 shuffle: False\n",
      "epoch: 4 total_correct 53076 loss: 143.67131759226322\n",
      "_______________________________________________________\n",
      "run id: 7\n",
      "batch_size: 32 lr: 0.001 shuffle: True\n",
      "epoch: 0 total_correct 45596 loss: 1201.2051393687725\n",
      "batch_size: 32 lr: 0.001 shuffle: True\n",
      "epoch: 1 total_correct 50778 loss: 790.7222563624382\n",
      "batch_size: 32 lr: 0.001 shuffle: True\n",
      "epoch: 2 total_correct 51986 loss: 679.7482296600938\n",
      "batch_size: 32 lr: 0.001 shuffle: True\n",
      "epoch: 3 total_correct 52760 loss: 608.817159358412\n",
      "batch_size: 32 lr: 0.001 shuffle: True\n",
      "epoch: 4 total_correct 53327 loss: 566.6983906719834\n",
      "_______________________________________________________\n",
      "run id: 8\n",
      "batch_size: 32 lr: 0.001 shuffle: False\n",
      "epoch: 0 total_correct 45577 loss: 1170.3946912139654\n",
      "batch_size: 32 lr: 0.001 shuffle: False\n",
      "epoch: 1 total_correct 50806 loss: 785.7423983290792\n",
      "batch_size: 32 lr: 0.001 shuffle: False\n",
      "epoch: 2 total_correct 52096 loss: 672.2698755264282\n",
      "batch_size: 32 lr: 0.001 shuffle: False\n",
      "epoch: 3 total_correct 52846 loss: 608.8436600416899\n",
      "batch_size: 32 lr: 0.001 shuffle: False\n",
      "epoch: 4 total_correct 53279 loss: 563.272816374898\n",
      "_______________________________________________________\n",
      "run id: 9\n",
      "batch_size: 64 lr: 0.001 shuffle: True\n",
      "epoch: 0 total_correct 43256 loss: 684.530079215765\n",
      "batch_size: 64 lr: 0.001 shuffle: True\n",
      "epoch: 1 total_correct 49465 loss: 450.8645385056734\n",
      "batch_size: 64 lr: 0.001 shuffle: True\n",
      "epoch: 2 total_correct 51179 loss: 381.9663436859846\n",
      "batch_size: 64 lr: 0.001 shuffle: True\n",
      "epoch: 3 total_correct 52031 loss: 344.0405369102955\n",
      "batch_size: 64 lr: 0.001 shuffle: True\n",
      "epoch: 4 total_correct 52536 loss: 319.25524624437094\n",
      "_______________________________________________________\n",
      "run id: 10\n",
      "batch_size: 64 lr: 0.001 shuffle: False\n",
      "epoch: 0 total_correct 43679 loss: 672.3036105632782\n",
      "batch_size: 64 lr: 0.001 shuffle: False\n",
      "epoch: 1 total_correct 49702 loss: 442.04953046143055\n",
      "batch_size: 64 lr: 0.001 shuffle: False\n",
      "epoch: 2 total_correct 51260 loss: 375.9611049294472\n",
      "batch_size: 64 lr: 0.001 shuffle: False\n",
      "epoch: 3 total_correct 52007 loss: 341.76240111887455\n",
      "batch_size: 64 lr: 0.001 shuffle: False\n",
      "epoch: 4 total_correct 52544 loss: 318.7411871701479\n",
      "_______________________________________________________\n",
      "run id: 11\n",
      "batch_size: 128 lr: 0.001 shuffle: True\n",
      "epoch: 0 total_correct 41189 loss: 395.80707371234894\n",
      "batch_size: 128 lr: 0.001 shuffle: True\n",
      "epoch: 1 total_correct 47406 loss: 260.98605954647064\n",
      "batch_size: 128 lr: 0.001 shuffle: True\n",
      "epoch: 2 total_correct 49464 loss: 222.87295973300934\n",
      "batch_size: 128 lr: 0.001 shuffle: True\n",
      "epoch: 3 total_correct 50658 loss: 198.0675223916769\n",
      "batch_size: 128 lr: 0.001 shuffle: True\n",
      "epoch: 4 total_correct 51444 loss: 181.80890966951847\n",
      "_______________________________________________________\n",
      "run id: 12\n",
      "batch_size: 128 lr: 0.001 shuffle: False\n",
      "epoch: 0 total_correct 41892 loss: 379.0085064172745\n",
      "batch_size: 128 lr: 0.001 shuffle: False\n",
      "epoch: 1 total_correct 48263 loss: 246.12710437178612\n",
      "batch_size: 128 lr: 0.001 shuffle: False\n",
      "epoch: 2 total_correct 50272 loss: 210.7830304801464\n",
      "batch_size: 128 lr: 0.001 shuffle: False\n",
      "epoch: 3 total_correct 51354 loss: 188.39967447519302\n",
      "batch_size: 128 lr: 0.001 shuffle: False\n",
      "epoch: 4 total_correct 52027 loss: 173.03900229930878\n",
      "_______________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for run_id, (lr, batch_size, shuffle) in enumerate(product(*param_values)):\n",
    "    print(\"run id:\", run_id + 1)\n",
    "    model = CNN().to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=shuffle)\n",
    "    optimizer = opt.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    comment = f' batch_size = {batch_size} lr = {lr} shuffle = {shuffle}'\n",
    "    tb = SummaryWriter(comment=comment)\n",
    "    for epoch in range(5):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            preds = model(images)\n",
    "\n",
    "            loss = criterion(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        tb.add_scalar(\"Loss\", total_loss, epoch)\n",
    "        tb.add_scalar(\"Correct\", total_correct, epoch)\n",
    "        tb.add_scalar(\"Accuracy\", total_correct/len(train_set), epoch)\n",
    "\n",
    "        print(\"batch_size:\", batch_size, \"lr:\", lr, \"shuffle:\", shuffle)\n",
    "        print(\"epoch:\", epoch, \"total_correct\", total_correct, \"loss:\", total_loss)\n",
    "    \n",
    "    print(\"_______________________________________________________\")\n",
    "        \n",
    "    tb.add_hparams(\n",
    "        {\"lr\":lr, \"bsize\":batch_size, \"shuffle\":shuffle}, # hyperparameters\n",
    "        {\n",
    "            \"accuracy\":total_correct/len(train_set),\n",
    "            \"loss\":total_loss,\n",
    "        }, # evaluation metrics\n",
    "    )\n",
    "\n",
    "tb.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8555e069dcf590ed4b4bae24bf467f4448bb388a231e28f14d8eee4760b976c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
